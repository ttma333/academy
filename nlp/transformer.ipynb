{"cells":[{"cell_type":"markdown","metadata":{"id":"5PUseM_N42in"},"source":["이 노트북은 [케라스 창시자에게 배우는 딥러닝 2판](https://tensorflow.blog/kerasdl2/)의 예제 코드를 담고 있습니다.\n","\n","<table align=\"left\">\n","    <tr>\n","        <td>\n","            <a href=\"https://colab.research.google.com/github/rickiepark/deep-learning-with-python-2nd/blob/main/chapter11_part03_transformer.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>\n","        </td>\n","    </tr>\n","</table>"]},{"cell_type":"markdown","metadata":{"id":"NhOmioaR42iq"},"source":["## 트랜스포머 아키텍처"]},{"cell_type":"markdown","metadata":{"id":"xWSmzK5142ir"},"source":["https://ratsgo.github.io/nlpbook/docs/language_model/tr_self_attention/\n","\n","### 셀프 어텐션 이해하기\n","\n","- 트랜스포머 구조에서 멀티 헤드 어텐션은 셀프 어텐션(self attention)이라고도 불립니다. \n","- 트랜스포머 경쟁력의 원천은 셀프 어텐션에 있다.\n","- 어텐션(attention)은 시퀀스 입력에 수행하는 기계학습 방법의 일종으로 시퀀스 요소들 가운데 태스크 수행에 중요한 요소에 집중하고 그렇지 않은 요소는 무시해 태스크 수행 성능을 끌어 올리며 기계 번역 과제에 처음 도입되었다.\n","- 기계 번역에 어텐션을 도입한다면 타깃 언어를 디코딩할 때 소스 언어의 단어 시퀀스 가운데 디코딩에 도움되는 단어들 위주로 취사 선택해서 번역 품질을 끌어 올리게 된다. 즉, 어텐션은 디코딩할 때 소스 시퀀스 가운데 중요한 요소들만 추린다.\n","- 셀프 어텐션이란, 말 그대로 자기 자신에 수행하는 어텐션 기법으로 입력 시퀀스 가운데 태스크 수행에 의미 있는 요소들 위주로 정보를 추출한다는 것이다.\n","- 합성곱 신경망과 비교 : CNN은 합성곱 필터(convoltion filter) 라는 특수한 장치를 이용해 시퀀스의 지역적인 특징을 잡아내는 모델이다. 자연어는 기본적으로 시퀀스(단어 혹은 형태소의 나열)이고 특정 단어 기준 주변 문맥이 의미 형성에 중요한 역할을 하므로 CNN이 자연어 처리에 널리 쓰인다.하지만 CNN은 합성곱 필터 크기를 넘어서는 문맥은 읽어내기 어렵다는 단점이 있습니다. 예컨대 필터 크기가 3(3개 단어씩 처리)이라면 4칸 이상 떨어져 있는 단어 사이의 의미는 캐치하기 어렵다.\n","- 순환 신경망과 비교 : RNN 역시 시퀀스 정보를 압축하는 데 강점이 있는 구조이다. 소스 언어 시퀀스인 어제, 카페, 갔었어, 거기, 사람, 많더라를 인코딩해야 한다고 가정해 보면 RNN은 소스 시퀀스를 차례대로 처리한다. 하지만 RNN은 시퀀스 길이가 길어질 수록 정보 압축에 문제가 발생하며 오래 전에 입력된 단어는 잊어버리거나, 특정 단어 정보를 과도하게 반영해 전체 정보를 왜곡하는 경우가 자주 생긴다.\n","기계 번역을 할 때 RNN을 사용한다면 인코더가 디코더로 넘기는 정보는 소스 시퀀스의 마지막인 많더라라는 단어의 의미가 많이 반영될 수밖에 없으며 RNN은 입력 정보를 차례대로 처리하고 오래 전에 읽었던 단어는 잊어버리는 경향이 있다.\n","- 어텐션과 비교 : cafe에 대응하는 소스 언어의 단어는 카페이고 이는 소스 시퀀스의 초반부에 등장한 상황에서 cafe라는 단어를 디코딩해야 할 때 카페를 반드시 참조해야 한다. 어텐션이 없는 단순 RNN을 사용하면 워낙 초반에 입력된 단어라 모델이 잊었을 가능성이 크고, 이 때문에 번역 품질이 낮아질 수 있다. 어텐션은 이러한 문제점을 해결하기 위해 제안되었으며 디코더 쪽 RNN에 어텐션을 추가하는 방식이다. 어텐션은 디코더가 타깃 시퀀스를 생성할 때 소스 시퀀스 전체에서 어떤 요소에 주목해야 할지 알려주므로 카페가 소스 시퀀스 초반에 등장하거나 소스 시퀀스의 길이가 길어지더라도 번역 품질이 떨어지는 것을 막을 수 있다.\n","- 셀프 어텐션은 자기 자신에 수행하는 어텐션이다.입력 시퀀스가 어제, 카페, 갔었어, 거기, 사람, 많더라일 때 거기라는 단어가 어떤 의미를 가지는지 계산하는 상황에서 잘 학습된 셀프 어텐션 모델이라면 거기에 대응하는 장소는 카페라는 사실을 알아챌 수 있다. 그뿐만 아니라 거기는 갔었어와도 연관이 있음을 확인할 수 있다. 트랜스포머 인코더 블록 내부에서는 이처럼 거기라는 단어를 인코딩할 때 카페, 갔었어라는 단어의 의미를 강조해서 반영한다.\n","- 셀프 어텐션 수행 대상은 입력 시퀀스 전체이며 개별 단어와 전체 입력 시퀀스를 대상으로 어텐션 계산을 수행해 문맥 전체를 고려하기 때문에 지역적인 문맥만 보는 CNN 대비 강점이 있다. 아울러 모든 경우의 수를 고려(단어들 서로가 서로를 1대 1로 바라보게 함)하기 때문에 시퀀스 길이가 길어지더라도 정보를 잊거나 왜곡할 염려가 없다. 이는 RNN의 단점을 극복한 지점입이다.\n","\n","어텐션과 셀프 어텐션의 주요 차이\n","- 어텐션은 소스 시퀀스 전체 단어들(어제, 카페, …, 많더라)과 타깃 시퀀스 단어 하나(cafe) 사이를 연결하는 데 쓰인다. 반면 셀프 어텐션은 입력 시퀀스 전체 단어들(그림15, 그림16) 사이를 연결한다.\n","- 어텐션은 RNN 구조 위에서 동작하지만 셀프 어텐션은 RNN 없이 동작한다.\n","타깃 언어의 단어를 1개 생성할 때 어텐션은 1회 수행하지만 셀프어텐션은 인코더, 디코더 블록의 개수만큼 반복 수행한다.\n"]},{"cell_type":"markdown","metadata":{"id":"2XdzrCS-42ir"},"source":["#### 일반화된 셀프 어텐션: 쿼리-키-값 모델\n","\n","- 셀프 어텐션은 쿼리(query), 키(key), 밸류(value) 세 가지 요소가 서로 영향을 주고 받는 구조이다. 트랜스포머 블록에는 문장 내 각 단어가 벡터(vector) 형태로 입력되는데 여기서 벡터란 숫자의 나열 정도로 이해할 수 있다.\n","- 각 단어 벡터는 블록 내에서 어떤 계산 과정을 거쳐 쿼리, 키, 밸류 세 가지로 변환된다. 만일 트랜스포머 블록에 입력되는 문장이 여섯 개 단어로 구성돼 있다면 이 블록의 셀프 어텐션 계산 대상은 쿼리 벡터 6개, 키 벡터 6개, 밸류 백터 6개 등 모두 18개가 된다. 셀프 어텐션은 쿼리 단어 각각에 대해 모든 키 단어와 얼마나 유기적인 관계를 맺고 있는지 그 합이 1인 확률값으로 나타낸다. 카페라는 쿼리 단어와 가장 관련이 높은 키 단어는 거기라는 점(0.4)을 확인할 수 있다.\n","- 셀프 어텐션 모듈은 밸류 벡터들을 가중합(weighted sum)하는 방식으로 계산을 마무리한다. 새롭게 만들어지는 카페 벡터( Z카페 )는 문장에 속한 모든 단어 쌍 사이의 관계가 녹아 있다.\n","Z카페=0.1×V어제+0.1×V카페+0.1×V갔었어+0.4×V거기+0.2×V사람+0.1×V많더라\n","- 카페에 대해서만 계산 예를 들었지만 이러한 방식으로 나머지 단어들도 셀프 어텐션을 각각 수행한다. 모드 시퀀스를 대상으로 셀프 어텐션 계산이 끝나면 그 결과를 다음 블록으로 넘깁니다. 이처럼 트랜스포머 모델은 셀프 어텐션을 블록(레이어) 수만큼 반복합니다."]},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/gdrive', force_remount=True)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ju0CIYuAVpFV","executionInfo":{"status":"ok","timestamp":1672929048669,"user_tz":-540,"elapsed":25392,"user":{"displayName":"kevin park","userId":"02703084888761299921"}},"outputId":"eee8aadc-e455-4aec-a459-bf68c384fba2"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /gdrive\n"]}]},{"cell_type":"markdown","source":["셀프 어텐션 동작 원리\n","\n","https://ratsgo.github.io/nlpbook/docs/language_model/tr_self_attention/"],"metadata":{"id":"CrTdajnYT8ED"}},{"cell_type":"markdown","source":["셀프 어텐션 출력 과정 : 쿼리, 키, 밸류 3개 요소 사이의 문맥적 관계성을 추출하는 과정\n","- 입력(단어 개수, 단어 임베딩 차원 수)\n","- 쿼리, 키, 밸류 만들기 : 세가지 행렬은 태스크를 가장 잘 수행하는 방향으로 학습 과정에서 업데이트\n","- 셀프 어텐션 계산\n","  - 쿼리 벡터들을 한꺼번에 모아서 키 벡터들과 행렬곱을 수행\n","  - 소프트맥스 확률값 만들기 : 키 벡터의 차원수의 제곱근으로 나눠준 뒤 소프트맥스를 취하는 과정\n","  - 소프트맥스 확률과 밸류를 가중합하기"],"metadata":{"id":"845aDaOi0Mo-"}},{"cell_type":"markdown","source":["셀프 어텐션 계산\n","\n","쿼리와 키를 행렬곱한 뒤 해당 행렬의 모든 요소값을 키 차원수의 제곱근 값으로 나눠주고, 이 행렬을 행(row) 단위로 소프트맥스(softmax)*를 취해 스코어 행렬을 만들어줍니다. 이 스코어 행렬에 밸류를 행렬곱해 줘서 셀프 어텐션 계산"],"metadata":{"id":"YBfcpu1A2gfh"}},{"cell_type":"markdown","source":[],"metadata":{"id":"AMQypMlW29dJ"}},{"cell_type":"code","source":["# 변수 정의\n","import torch\n","\n","# 입력 벡타 시퀀스(단어 개수 3, 차원 수 4)\n","x = torch.tensor([\n","  [1.0, 0.0, 1.0, 0.0],\n","  [0.0, 2.0, 0.0, 2.0],\n","  [1.0, 1.0, 1.0, 1.0],  \n","])\n","\n","# 셀프 어텐션은 쿼리, 키, 밸류 3개 요소 사이의 문맥적 관계성을 추출하는 과정\n","# 쿼리, 키, 밸류를 만들어 주는 행렬(w)\n","w_query = torch.tensor([\n","  [1.0, 0.0, 1.0],\n","  [1.0, 0.0, 0.0],\n","  [0.0, 0.0, 1.0],\n","  [0.0, 1.0, 1.0]\n","])\n","w_key = torch.tensor([\n","  [0.0, 0.0, 1.0],\n","  [1.0, 1.0, 0.0],\n","  [0.0, 1.0, 0.0],\n","  [1.0, 1.0, 0.0]\n","])\n","w_value = torch.tensor([\n","  [0.0, 2.0, 0.0],\n","  [0.0, 3.0, 0.0],\n","  [1.0, 0.0, 3.0],\n","  [1.0, 1.0, 0.0]\n","])"],"metadata":{"id":"4MswquQiT2a_"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# 쿼리, 키, 밸류 만들기\n","# w_key,w_query,w_value 세가지 행렬은 태스크를 가장 잘 수행할 수 있는 방향으로 학습 과정에서 업데이트\n","keys = torch.matmul(x, w_key)\n","querys = torch.matmul(x, w_query)\n","values = torch.matmul(x, w_value)\n","print(keys,'\\n')\n","print(querys,'\\n')\n","print(values)"],"metadata":{"id":"rBESrrUoWP6R","executionInfo":{"status":"ok","timestamp":1673137482697,"user_tz":-540,"elapsed":637,"user":{"displayName":"kevin park","userId":"02703084888761299921"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"ba75c81d-5d69-40c9-d05e-e4d5f51d4112"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["tensor([[0., 1., 1.],\n","        [4., 4., 0.],\n","        [2., 3., 1.]]) \n","\n","tensor([[1., 0., 2.],\n","        [2., 2., 2.],\n","        [2., 1., 3.]]) \n","\n","tensor([[1., 2., 3.],\n","        [2., 8., 0.],\n","        [2., 6., 3.]])\n"]}]},{"cell_type":"code","source":["#  쿼리 벡터들을 한꺼번에 모아서 키 벡터들과 행렬곱을 수행\n","attn_scores = torch.matmul(querys, keys.T)\n","print(querys,'\\n')\n","print(keys.T,'\\n')\n","attn_scores"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"CJkPgBQ-WVBr","executionInfo":{"status":"ok","timestamp":1673137594372,"user_tz":-540,"elapsed":467,"user":{"displayName":"kevin park","userId":"02703084888761299921"}},"outputId":"916e7f96-e10a-44b0-fb33-524615894668"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["tensor([[1., 0., 2.],\n","        [2., 2., 2.],\n","        [2., 1., 3.]]) \n","\n","tensor([[0., 4., 2.],\n","        [1., 4., 3.],\n","        [1., 0., 1.]]) \n","\n"]},{"output_type":"execute_result","data":{"text/plain":["tensor([[ 2.,  4.,  4.],\n","        [ 4., 16., 12.],\n","        [ 4., 12., 10.]])"]},"metadata":{},"execution_count":4}]},{"cell_type":"code","source":["import numpy as np\n","print(keys,'\\n')\n","print(keys.shape[-1],'\\n')\n","np.sqrt(keys.shape[-1])"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"8lFMHdBxygRV","executionInfo":{"status":"ok","timestamp":1673138019775,"user_tz":-540,"elapsed":14,"user":{"displayName":"kevin park","userId":"02703084888761299921"}},"outputId":"bcc754cb-6717-4ec8-c426-5615e9e4bdea"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["tensor([[0., 1., 1.],\n","        [4., 4., 0.],\n","        [2., 3., 1.]]) \n","\n","3 \n","\n"]},{"output_type":"execute_result","data":{"text/plain":["1.7320508075688772"]},"metadata":{},"execution_count":10}]},{"cell_type":"code","source":["# 소프트맥스 확률값 만들기 : 키 벡터의 차원수의 제곱근으로 나눠준 뒤 소프트맥스를 취하는 과정\n","import numpy as np\n","from torch.nn.functional import softmax\n","key_dim_sqrt = np.sqrt(keys.shape[-1])\n","attn_probs = softmax(attn_scores / key_dim_sqrt, dim=-1)\n","attn_probs"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"kz4CHBcsWnEJ","executionInfo":{"status":"ok","timestamp":1673138188219,"user_tz":-540,"elapsed":466,"user":{"displayName":"kevin park","userId":"02703084888761299921"}},"outputId":"6adb629e-4b33-4f4e-fdbc-6f7e1bf0c1d2"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([[1.3613e-01, 4.3194e-01, 4.3194e-01],\n","        [8.9045e-04, 9.0884e-01, 9.0267e-02],\n","        [7.4449e-03, 7.5471e-01, 2.3785e-01]])"]},"metadata":{},"execution_count":13}]},{"cell_type":"code","source":["# 소프트맥스 확률과 밸류를 가중합하기\n","weighted_values = torch.matmul(attn_probs, values)\n","print(values,'\\n')\n","weighted_values"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"uK03Bu-9W0Et","executionInfo":{"status":"ok","timestamp":1673138230368,"user_tz":-540,"elapsed":506,"user":{"displayName":"kevin park","userId":"02703084888761299921"}},"outputId":"6126f78d-bd6c-4516-ecbc-4602f3a17f0c"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["tensor([[1., 2., 3.],\n","        [2., 8., 0.],\n","        [2., 6., 3.]]) \n","\n"]},{"output_type":"execute_result","data":{"text/plain":["tensor([[1.8639, 6.3194, 1.7042],\n","        [1.9991, 7.8141, 0.2735],\n","        [1.9926, 7.4796, 0.7359]])"]},"metadata":{},"execution_count":14}]},{"cell_type":"markdown","source":["- 셀프 어텐션의 학습 대상은 쿼리, 키, 밸류를 만드는 가중치 행렬입니다. \n","- 코드 예시에서는 w_query, w_key, w_value입니다. \n","- 이들은 태스크(예: 기계 번역)를 가장 잘 수행하는 방향으로 학습 과정에서 업데이트"],"metadata":{"id":"a9kNkM6SZCvR"}},{"cell_type":"markdown","metadata":{"id":"Qo8cK-c_42ir"},"source":["### 멀티 헤드 어텐션\n","\n","- 셀프 어텐션(self attention)을 여러 번 수행한 걸 가리킵니다. \n","- 여러 헤드가 독자적으로 셀프 어텐션을 계산한다는 이야기입니다. 비유하자면 같은 문서(입력)를 두고 독자(헤드) 여러 명이 함께 읽는 구조\n","- 멀티-헤드 어텐션은 개별 헤드의 셀프 어텐션 수행 결과를 이어붙인 행렬에  WO 를 행렬곱해서 마무리\n","- 멀티-헤드 어텐션의 최종 수행 결과는 ‘입력 단어 수  ×  목표 차원수’\n","- 멀티 헤드 어텐션은 인코더, 디코더 블록 모두에 적용됩니다. 앞으로 특별한 언급이 없다면 셀프 어텐션은 멀티 헤드 어텐션인 것으로 이해"]},{"cell_type":"markdown","source":["트랜스포머 모델 학습 및 인퍼런스 : 기계 번역을 수행(인퍼런스)\n"," - 소스 언어(한국어) 문장을 인코더에 입력해 인코더 마지막 블록의 단어 벡터 시퀀스를 추출\n"," - 인코더에서 넘어온 소스 언어 문장 정보와 디코더에 타깃 문장 시작을 알리는 스페셜 토큰 $<s>$를 넣어서, 타깃 언어(영어)의 첫 번째 토큰을 생성\n"," - 인코더 쪽에서 넘어온 소스 언어 문장 정보와 이전에 생성된 타깃 언어 토큰 시퀀스를 디코더에 넣어서 만든 정보로 타깃 언어의 다음 토큰을 생성\n"," - 생성된 문장 길이가 충분하거나 문장 끝을 알리는 스페셜 토큰 $</s>$가 나올 때까지 3을 반복"],"metadata":{"id":"JzxDQp149Qtq"}},{"cell_type":"markdown","source":["트랜스포머에 적용된 기술\n","- 인코더와 디코더 블록의 구조는 멀티 헤드 어텐션, 피드포워드 뉴럴 네트워크, 잔차 연결 및 레이어 정규화 등 세 가지 구성 요소를 기본\n","- 피드포워드 뉴럴네트워크\n","  - 입력은 현재 블록의 멀티 헤드 어텐션의의 개별 출력 벡터\n","  - 피드포워드 뉴럴네트워크란 신경망(neural network)의 한 종류로 그림2와 같이 입력층(input layer,  x ), 은닉층(hidden layer,  h ), 출력층(ouput layer,  y ) 3개 계층으로 구성\n","  - 이전 뉴런 값과 그에 해당하는 가중치를 가중합(weighted sum)한 결과에 바이어스(bias)를 더해 만듭니다. 가중치들과 바이어스는 학습 과정에서 업데이트. 활성 함수(activation function,  f )는 현재 계산하고 있는 뉴런의 출력을 일정 범위로 제한하는 역할. 활성함수는 ReLU(Rectified Linear Unit)\n","  - 입력층 뉴런이 각각  [2,1] 이고 그에 해당하는 가중치가  [3,2] , 바이어스(bias)가 1이라고 가정해 보겠습니다. 그러면 은닉층 첫번째 뉴런 값은  2×3+1×2+1=9 가 되며 이 값은 양수이므로 ReLU를 통과해도 그대로 유지\n","  - 트랜스포머에서는 은닉층의 뉴런 갯수(즉 은닉층의 차원수)를 입력층의 네 배로 설정. 예컨대 피드포워드 뉴럴네트워크의 입력 벡터가 768차원일 경우 은닉층은 2048차원까지 늘렸다가 출력층에서 이를 다시 768차원으로 줄인다.\n","\n","- 잔차 연결\n","  - 잔차 연결이란 블록(block) 계산을 건너뛰는 경로를 하나 두는 것을 의미\n","  - 입력을  x , 이번 계산 대상 블록을  F 라고 할 때 잔차 연결은  F(x)+x 로 간단히 실현\n","  - 잔차 연결을 두지 않았을 때는  f1 ,  f2 ,  f3 을 연속으로 수행하는 경로 한 가지만 존재하였으나, 잔차 연결을 블록마다 설정해둠으로써 모두 8가지의 새로운 경로가 생성. 다시 말해 모델이 다양한 관점에서 블록 계산을 수행\n","  - 잔차 연결은 모델 중간에 블록을 건너뛰는 경로를 설정함으로써 학습을 용이하게 하는 효과가 있음\n","\n","- 레이어 정규화\n","  - 미니 배치의 인스턴스( x )별로 평균을 빼주고 표준편차로 나눠줘 정규화(normalization)을 수행하는 기법\n","  - 레이어 정규화를 수행하면 학습이 안정되고 그 속도가 빨라지는 등의 효과\n","\n","- 드롭아웃\n","  - 딥러닝 모델은 그 표현력이 아주 좋아서 학습 데이터 그 자체를 외워버릴 염려가 있습니다. 이를 과적합(overfitting)이라고 합니다. 드롭아웃(dropout)은 이러한 과적합 현상을 방지하고자 뉴런의 일부를 확률적으로 0으로 대치하여 계산에서 제외하는 기법\n","  - torch.nn.Dropout 객체는 뉴런별로 드롭아웃을 수행할지 말지를 확률적으로 결정하는 함수인데 p=0.2라는 말은 드롭아웃 수행 비율이 평균적으로 20%가 되게끔 하는 것임\n","\n","- 옵티마이저\n","  - 딥러닝 모델 학습은 모델 출력과 정답 사이의 오차(error)를 최소화하는 방향을 구하고 이 방향에 맞춰 모델 전체의 파라미터(parameter)들을 업데이트하는 과정. 이때 오차를 최소화하는 방향을 그래디언트(gradient)라고 하며 오차를 최소화하는 과정을 최적화(optimization)라고 함\n","  - 오차를 구하려면 현재 시점의 모델에 입력을 넣어봐서 처음부터 끝까지 계산해보고 정답과 비교해야 하며 오차를 구하기 위해 이같이 모델 처음부터 끝까지 순서대로 계산해보는 과정을 순전파(forward propagation)이라고 한다.\n","  - 오차를 구했다면 오차를 최소화하는 최초의 그래디언트를 구할 수 있으며 이는 미분(devative)으로 구합니다. 이후 미분의 연쇄 법칙(chain rule)에 따라 모델 각 가중치별 그래디언트 역시 구할 수 있다. 이 과정은 순전파의 역순으로 순차적으로 수행되는데 이를 역전파(backpropagation)라고 함\n","  - 학습 과정은 미니 배치 단위로 이루어지며 이는 눈을 가린 상태에서 산등성이를 한걸음씩 내려가는 과정에 비유할 수 있다. 내가 지금 있는 위치에서 360도 모든 방향에 대해 한발한발 내딛어보고 가장 경사가 급한 쪽으로 한걸음씩 내려가는 과정을 반복하는 것이다.\n","  - 모델을 업데이트할 때(산등성이를 내려갈 때) 중요한 것은 방향과 보폭인데 이는 최적화 도구(optimizer)의 도움을 받는다. 트랜스포머 모델이 쓰는 최적화 도구가 바로 아담 옵티마이저(Adam Optimizer)이며 아담 옵티마이저는 오차를 줄이는 성능이 좋아서 트랜스포머 말고도 널리 쓰인다.\n","  - 아담 옵티마이저의 핵심 동작 원리는 방향과 보폭을 적절하게 정해주는 것이다. 방향을 정할 때는 현재 위치에서 가장 경사가 급한 쪽으로 내려가되, 여태까지 내려오던 관성(방향)을 일부 유지하도록 한다. 보폭의 경우 안가본 곳은 성큼 빠르게 걸어 훑고 많이 가본 곳은 갈수록 보폭을 줄여 세밀하게 탐색하는 방식으로 정한다."],"metadata":{"id":"6Ou43y3j-iBH"}},{"cell_type":"code","source":["import torch\n","x = torch.tensor([2,1])\n","w1 = torch.tensor([[3,2,-4],[2,-3,1]])\n","b1 = 1\n","w2 = torch.tensor([[-1, 1], [1,2], [3,1]])\n","b2 = -1"],"metadata":{"id":"mj2vGcxW_8Rh"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["h_preact = torch.matmul(x, w1) + b1\n","h = torch.nn.functional.relu(h_preact)\n","y = torch.matmul(h, w2) + b2\n","print(h_preact,'\\n')\n","print(h,'\\n')\n","y"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"yCH4GjzwABDi","executionInfo":{"status":"ok","timestamp":1673141481533,"user_tz":-540,"elapsed":452,"user":{"displayName":"kevin park","userId":"02703084888761299921"}},"outputId":"67c946c2-6900-4d03-bdcf-e1398b30cfd6"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["tensor([ 9,  2, -6]) \n","\n","tensor([9, 2, 0]) \n","\n"]},{"output_type":"execute_result","data":{"text/plain":["tensor([-8, 12])"]},"metadata":{},"execution_count":18}]},{"cell_type":"code","source":["# 레이어 정규화 예시\n","import torch\n","input = torch.tensor([[1.0, 2.0, 3.0], [1.0, 1.0, 1.0]])\n","m = torch.nn.LayerNorm(input.shape[-1])\n","output = m(input)\n","output"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"TR-DabLLAFNY","executionInfo":{"status":"ok","timestamp":1673141977016,"user_tz":-540,"elapsed":461,"user":{"displayName":"kevin park","userId":"02703084888761299921"}},"outputId":"9bb6dba5-18c3-4282-afe5-c1823e7bab0a"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([[-1.2247,  0.0000,  1.2247],\n","        [ 0.0000,  0.0000,  0.0000]], grad_fn=<NativeLayerNormBackward0>)"]},"metadata":{},"execution_count":20}]},{"cell_type":"code","source":["print(m.weight,'\\n')\n","print(m.bias)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"8V3MkAImCQ1C","executionInfo":{"status":"ok","timestamp":1673142053038,"user_tz":-540,"elapsed":470,"user":{"displayName":"kevin park","userId":"02703084888761299921"}},"outputId":"8ab9e270-f0ec-4fbc-8d44-1fbdaa2a07b6"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Parameter containing:\n","tensor([1., 1., 1.], requires_grad=True) \n","\n","Parameter containing:\n","tensor([0., 0., 0.], requires_grad=True)\n"]}]},{"cell_type":"markdown","metadata":{"id":"F0HqiH6C42ir"},"source":["### 트랜스포머 인코더"]},{"cell_type":"markdown","metadata":{"id":"LjYMcwAl42is"},"source":["**데이터 가져오기**"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"i7VVn1yl42is","outputId":"51a4ce13-cd7f-4ad2-e23c-a5fa301300e0","executionInfo":{"status":"ok","timestamp":1673140532258,"user_tz":-540,"elapsed":34784,"user":{"displayName":"kevin park","userId":"02703084888761299921"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["rm: cannot remove 'aclImdb': No such file or directory\n","  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n","                                 Dload  Upload   Total   Spent    Left  Speed\n","100 80.2M  100 80.2M    0     0  2824k      0  0:00:29  0:00:29 --:--:-- 1974k\n"]}],"source":["!rm -r aclImdb\n","!curl -O https://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\n","!tar -xf aclImdb_v1.tar.gz\n","!rm -r aclImdb/train/unsup"]},{"cell_type":"markdown","metadata":{"id":"D8aiOAjJ42iu"},"source":["**데이터 준비**"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"7C8nBL2242iu","outputId":"d596ddb2-1609-46d7-d59e-67731e04358d"},"outputs":[{"output_type":"stream","name":"stdout","text":["Found 20000 files belonging to 2 classes.\n","Found 5000 files belonging to 2 classes.\n","Found 25000 files belonging to 2 classes.\n"]}],"source":["import os, pathlib, shutil, random\n","from tensorflow import keras\n","batch_size = 32\n","base_dir = pathlib.Path(\"aclImdb\")\n","val_dir = base_dir / \"val\"\n","train_dir = base_dir / \"train\"\n","for category in (\"neg\", \"pos\"):\n","    os.makedirs(val_dir / category)\n","    files = os.listdir(train_dir / category)\n","    random.Random(1337).shuffle(files)\n","    num_val_samples = int(0.2 * len(files))\n","    val_files = files[-num_val_samples:]\n","    for fname in val_files:\n","        shutil.move(train_dir / category / fname,\n","                    val_dir / category / fname)\n","\n","train_ds = keras.utils.text_dataset_from_directory(\n","    \"aclImdb/train\", batch_size=batch_size\n",")\n","val_ds = keras.utils.text_dataset_from_directory(\n","    \"aclImdb/val\", batch_size=batch_size\n",")\n","test_ds = keras.utils.text_dataset_from_directory(\n","    \"aclImdb/test\", batch_size=batch_size\n",")\n","text_only_train_ds = train_ds.map(lambda x, y: x)"]},{"cell_type":"markdown","metadata":{"id":"e0MsIvhr42iv"},"source":["**데이터 벡터화**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"--rJ5PYw42iv"},"outputs":[],"source":["from tensorflow.keras import layers\n","\n","max_length = 600\n","max_tokens = 20000\n","text_vectorization = layers.TextVectorization(\n","    max_tokens=max_tokens,\n","    output_mode=\"int\",\n","    output_sequence_length=max_length,\n",")\n","text_vectorization.adapt(text_only_train_ds)\n","\n","int_train_ds = train_ds.map(\n","    lambda x, y: (text_vectorization(x), y),\n","    num_parallel_calls=4)\n","int_val_ds = val_ds.map(\n","    lambda x, y: (text_vectorization(x), y),\n","    num_parallel_calls=4)\n","int_test_ds = test_ds.map(\n","    lambda x, y: (text_vectorization(x), y),\n","    num_parallel_calls=4)"]},{"cell_type":"markdown","metadata":{"id":"uZrJUai142iv"},"source":["**`Layer` 층을 상속하여 구현한 트랜스포머 인코더**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"924jztFe42iv"},"outputs":[],"source":["import tensorflow as tf\n","from tensorflow import keras\n","from tensorflow.keras import layers\n","\n","class TransformerEncoder(layers.Layer):\n","    def __init__(self, embed_dim, dense_dim, num_heads, **kwargs):\n","        super().__init__(**kwargs)\n","        self.embed_dim = embed_dim\n","        self.dense_dim = dense_dim\n","        self.num_heads = num_heads\n","        self.attention = layers.MultiHeadAttention(\n","            num_heads=num_heads, key_dim=embed_dim)\n","        self.dense_proj = keras.Sequential(\n","            [layers.Dense(dense_dim, activation=\"relu\"),\n","             layers.Dense(embed_dim),]\n","        )\n","        self.layernorm_1 = layers.LayerNormalization()\n","        self.layernorm_2 = layers.LayerNormalization()\n","\n","    def call(self, inputs, mask=None):\n","        if mask is not None:\n","            mask = mask[:, tf.newaxis, :]\n","        attention_output = self.attention(\n","            inputs, inputs, attention_mask=mask)\n","        proj_input = self.layernorm_1(inputs + attention_output)\n","        proj_output = self.dense_proj(proj_input)\n","        return self.layernorm_2(proj_input + proj_output)\n","\n","    def get_config(self):\n","        config = super().get_config()\n","        config.update({\n","            \"embed_dim\": self.embed_dim,\n","            \"num_heads\": self.num_heads,\n","            \"dense_dim\": self.dense_dim,\n","        })\n","        return config"]},{"cell_type":"markdown","metadata":{"id":"8yaSXFTO42iw"},"source":["**트랜스포머 인코더를 사용하여 텍스트 분류하기**"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"-Nk9lx8A42iw","outputId":"a0d29e8c-b21a-4139-a2ff-00e6c03fb1b7"},"outputs":[{"output_type":"stream","name":"stdout","text":["Model: \"model\"\n","_________________________________________________________________\n"," Layer (type)                Output Shape              Param #   \n","=================================================================\n"," input_1 (InputLayer)        [(None, None)]            0         \n","                                                                 \n"," embedding (Embedding)       (None, None, 256)         5120000   \n","                                                                 \n"," transformer_encoder (Transf  (None, None, 256)        543776    \n"," ormerEncoder)                                                   \n","                                                                 \n"," global_max_pooling1d (Globa  (None, 256)              0         \n"," lMaxPooling1D)                                                  \n","                                                                 \n"," dropout (Dropout)           (None, 256)               0         \n","                                                                 \n"," dense_2 (Dense)             (None, 1)                 257       \n","                                                                 \n","=================================================================\n","Total params: 5,664,033\n","Trainable params: 5,664,033\n","Non-trainable params: 0\n","_________________________________________________________________\n"]}],"source":["vocab_size = 20000\n","embed_dim = 256\n","num_heads = 2\n","dense_dim = 32\n","\n","inputs = keras.Input(shape=(None,), dtype=\"int64\")\n","x = layers.Embedding(vocab_size, embed_dim)(inputs)\n","x = TransformerEncoder(embed_dim, dense_dim, num_heads)(x)\n","x = layers.GlobalMaxPooling1D()(x)\n","x = layers.Dropout(0.5)(x)\n","outputs = layers.Dense(1, activation=\"sigmoid\")(x)\n","model = keras.Model(inputs, outputs)\n","model.compile(optimizer=\"rmsprop\",\n","              loss=\"binary_crossentropy\",\n","              metrics=[\"accuracy\"])\n","model.summary()"]},{"cell_type":"markdown","metadata":{"id":"7nXp84h_42iw"},"source":["**트랜스포머 인코더 기반 모델 훈련하고 평가하기**"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"E3H26gQC42ix","outputId":"7570856c-1009-49bf-84d5-235b477c52a1"},"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 1/20\n","625/625 [==============================] - 45s 63ms/step - loss: 0.4759 - accuracy: 0.7742 - val_loss: 0.3358 - val_accuracy: 0.8582\n","Epoch 2/20\n","625/625 [==============================] - 40s 64ms/step - loss: 0.3128 - accuracy: 0.8675 - val_loss: 0.2905 - val_accuracy: 0.8796\n","Epoch 3/20\n","625/625 [==============================] - 41s 65ms/step - loss: 0.2428 - accuracy: 0.9009 - val_loss: 0.2768 - val_accuracy: 0.8874\n","Epoch 4/20\n","625/625 [==============================] - 42s 66ms/step - loss: 0.1935 - accuracy: 0.9252 - val_loss: 0.2777 - val_accuracy: 0.8906\n","Epoch 5/20\n","625/625 [==============================] - 42s 66ms/step - loss: 0.1538 - accuracy: 0.9416 - val_loss: 0.2862 - val_accuracy: 0.8948\n","Epoch 6/20\n","625/625 [==============================] - 41s 65ms/step - loss: 0.1289 - accuracy: 0.9523 - val_loss: 0.3030 - val_accuracy: 0.8852\n","Epoch 7/20\n","625/625 [==============================] - 41s 66ms/step - loss: 0.1093 - accuracy: 0.9589 - val_loss: 0.3418 - val_accuracy: 0.8874\n","Epoch 8/20\n","625/625 [==============================] - 41s 66ms/step - loss: 0.0957 - accuracy: 0.9653 - val_loss: 0.4081 - val_accuracy: 0.8754\n","Epoch 9/20\n","625/625 [==============================] - 41s 65ms/step - loss: 0.0813 - accuracy: 0.9712 - val_loss: 0.4589 - val_accuracy: 0.8746\n","Epoch 10/20\n","625/625 [==============================] - 41s 66ms/step - loss: 0.0706 - accuracy: 0.9758 - val_loss: 0.6643 - val_accuracy: 0.8584\n","Epoch 11/20\n","625/625 [==============================] - 41s 65ms/step - loss: 0.0622 - accuracy: 0.9790 - val_loss: 0.5812 - val_accuracy: 0.8668\n","Epoch 12/20\n","625/625 [==============================] - 42s 66ms/step - loss: 0.0503 - accuracy: 0.9828 - val_loss: 0.7318 - val_accuracy: 0.8546\n","Epoch 13/20\n","625/625 [==============================] - 41s 65ms/step - loss: 0.0402 - accuracy: 0.9870 - val_loss: 0.7936 - val_accuracy: 0.8434\n","Epoch 14/20\n","625/625 [==============================] - 40s 65ms/step - loss: 0.0366 - accuracy: 0.9883 - val_loss: 0.8799 - val_accuracy: 0.8554\n","Epoch 15/20\n","625/625 [==============================] - 42s 66ms/step - loss: 0.0296 - accuracy: 0.9898 - val_loss: 0.8550 - val_accuracy: 0.8410\n","Epoch 16/20\n","625/625 [==============================] - 41s 65ms/step - loss: 0.0225 - accuracy: 0.9930 - val_loss: 0.8013 - val_accuracy: 0.8530\n","Epoch 17/20\n","625/625 [==============================] - 42s 66ms/step - loss: 0.0212 - accuracy: 0.9929 - val_loss: 0.9202 - val_accuracy: 0.8640\n","Epoch 18/20\n","625/625 [==============================] - 41s 65ms/step - loss: 0.0173 - accuracy: 0.9945 - val_loss: 0.9175 - val_accuracy: 0.8508\n","Epoch 19/20\n","625/625 [==============================] - 42s 66ms/step - loss: 0.0138 - accuracy: 0.9959 - val_loss: 1.0007 - val_accuracy: 0.8642\n","Epoch 20/20\n","625/625 [==============================] - 40s 64ms/step - loss: 0.0139 - accuracy: 0.9965 - val_loss: 1.1976 - val_accuracy: 0.8554\n","782/782 [==============================] - 22s 27ms/step - loss: 0.2754 - accuracy: 0.8869\n","테스트 정확도: 0.887\n"]}],"source":["callbacks = [\n","    keras.callbacks.ModelCheckpoint(\"transformer_encoder.keras\",\n","                                    save_best_only=True)\n","]\n","model.fit(int_train_ds, validation_data=int_val_ds, epochs=20, callbacks=callbacks)\n","model = keras.models.load_model(\n","    \"transformer_encoder.keras\",\n","    custom_objects={\"TransformerEncoder\": TransformerEncoder})\n","print(f\"테스트 정확도: {model.evaluate(int_test_ds)[1]:.3f}\")"]},{"cell_type":"markdown","metadata":{"id":"Ffb0_42j42ix"},"source":["#### 위치 인코딩을 사용해 위치 정보 주입하기"]},{"cell_type":"markdown","metadata":{"id":"c3eCKYxO42ix"},"source":["**서브클래싱으로 위치 임베딩 구현하기**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"7ngZXecb42ix"},"outputs":[],"source":["class PositionalEmbedding(layers.Layer):\n","    def __init__(self, sequence_length, input_dim, output_dim, **kwargs):\n","        super().__init__(**kwargs)\n","        self.token_embeddings = layers.Embedding(\n","            input_dim=input_dim, output_dim=output_dim)\n","        self.position_embeddings = layers.Embedding(\n","            input_dim=sequence_length, output_dim=output_dim)\n","        self.sequence_length = sequence_length\n","        self.input_dim = input_dim\n","        self.output_dim = output_dim\n","\n","    def call(self, inputs):\n","        length = tf.shape(inputs)[-1]\n","        positions = tf.range(start=0, limit=length, delta=1)\n","        embedded_tokens = self.token_embeddings(inputs)\n","        embedded_positions = self.position_embeddings(positions)\n","        return embedded_tokens + embedded_positions\n","\n","    def compute_mask(self, inputs, mask=None):\n","        return tf.math.not_equal(inputs, 0)\n","\n","    def get_config(self):\n","        config = super().get_config()\n","        config.update({\n","            \"output_dim\": self.output_dim,\n","            \"sequence_length\": self.sequence_length,\n","            \"input_dim\": self.input_dim,\n","        })\n","        return config"]},{"cell_type":"markdown","metadata":{"id":"YAOg6EBF42ix"},"source":["#### 텍스트 분류 트랜스포머"]},{"cell_type":"markdown","metadata":{"id":"9RXcbNDa42iy"},"source":["**트랜스포머 인코더와 위치 임베딩 합치기**"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"QmzXmO7042iy","outputId":"f8274129-c171-4206-fcb5-8a757d0daa2c"},"outputs":[{"output_type":"stream","name":"stdout","text":["Model: \"model_1\"\n","_________________________________________________________________\n"," Layer (type)                Output Shape              Param #   \n","=================================================================\n"," input_2 (InputLayer)        [(None, None)]            0         \n","                                                                 \n"," positional_embedding (Posit  (None, None, 256)        5273600   \n"," ionalEmbedding)                                                 \n","                                                                 \n"," transformer_encoder_1 (Tran  (None, None, 256)        543776    \n"," sformerEncoder)                                                 \n","                                                                 \n"," global_max_pooling1d_1 (Glo  (None, 256)              0         \n"," balMaxPooling1D)                                                \n","                                                                 \n"," dropout_1 (Dropout)         (None, 256)               0         \n","                                                                 \n"," dense_7 (Dense)             (None, 1)                 257       \n","                                                                 \n","=================================================================\n","Total params: 5,817,633\n","Trainable params: 5,817,633\n","Non-trainable params: 0\n","_________________________________________________________________\n","Epoch 1/20\n","625/625 [==============================] - 44s 68ms/step - loss: 0.4744 - accuracy: 0.7806 - val_loss: 0.2907 - val_accuracy: 0.8860\n","Epoch 2/20\n","625/625 [==============================] - 42s 67ms/step - loss: 0.2282 - accuracy: 0.9097 - val_loss: 0.2986 - val_accuracy: 0.8858\n","Epoch 3/20\n","625/625 [==============================] - 42s 67ms/step - loss: 0.1710 - accuracy: 0.9365 - val_loss: 0.3056 - val_accuracy: 0.8858\n","Epoch 4/20\n","625/625 [==============================] - 42s 67ms/step - loss: 0.1382 - accuracy: 0.9494 - val_loss: 0.3525 - val_accuracy: 0.8842\n","Epoch 5/20\n","625/625 [==============================] - 42s 67ms/step - loss: 0.1143 - accuracy: 0.9605 - val_loss: 0.3872 - val_accuracy: 0.8834\n","Epoch 6/20\n","625/625 [==============================] - 42s 67ms/step - loss: 0.0937 - accuracy: 0.9680 - val_loss: 0.4358 - val_accuracy: 0.8664\n","Epoch 7/20\n","625/625 [==============================] - 42s 67ms/step - loss: 0.0825 - accuracy: 0.9719 - val_loss: 0.4799 - val_accuracy: 0.8848\n","Epoch 8/20\n","625/625 [==============================] - 42s 67ms/step - loss: 0.0753 - accuracy: 0.9759 - val_loss: 0.4304 - val_accuracy: 0.8792\n","Epoch 9/20\n","625/625 [==============================] - 42s 68ms/step - loss: 0.0642 - accuracy: 0.9785 - val_loss: 0.5895 - val_accuracy: 0.8780\n","Epoch 10/20\n","625/625 [==============================] - 42s 67ms/step - loss: 0.0588 - accuracy: 0.9815 - val_loss: 0.6139 - val_accuracy: 0.8750\n","Epoch 11/20\n","625/625 [==============================] - 43s 69ms/step - loss: 0.0530 - accuracy: 0.9833 - val_loss: 0.5440 - val_accuracy: 0.8760\n","Epoch 12/20\n","625/625 [==============================] - 42s 67ms/step - loss: 0.0475 - accuracy: 0.9848 - val_loss: 0.5335 - val_accuracy: 0.8692\n","Epoch 13/20\n","625/625 [==============================] - 42s 68ms/step - loss: 0.0395 - accuracy: 0.9880 - val_loss: 0.6094 - val_accuracy: 0.8744\n","Epoch 14/20\n","625/625 [==============================] - 42s 67ms/step - loss: 0.0382 - accuracy: 0.9881 - val_loss: 0.6731 - val_accuracy: 0.8714\n","Epoch 15/20\n","625/625 [==============================] - 42s 67ms/step - loss: 0.0331 - accuracy: 0.9901 - val_loss: 0.8113 - val_accuracy: 0.8634\n","Epoch 16/20\n","625/625 [==============================] - 42s 67ms/step - loss: 0.0336 - accuracy: 0.9904 - val_loss: 0.6600 - val_accuracy: 0.8702\n","Epoch 17/20\n","625/625 [==============================] - 42s 67ms/step - loss: 0.0274 - accuracy: 0.9925 - val_loss: 0.8205 - val_accuracy: 0.8634\n","Epoch 18/20\n","625/625 [==============================] - 42s 67ms/step - loss: 0.0256 - accuracy: 0.9930 - val_loss: 0.7874 - val_accuracy: 0.8682\n","Epoch 19/20\n","625/625 [==============================] - 42s 67ms/step - loss: 0.0239 - accuracy: 0.9936 - val_loss: 0.8704 - val_accuracy: 0.8568\n","Epoch 20/20\n","625/625 [==============================] - 42s 67ms/step - loss: 0.0205 - accuracy: 0.9948 - val_loss: 0.8604 - val_accuracy: 0.8680\n","782/782 [==============================] - 20s 25ms/step - loss: 0.2910 - accuracy: 0.8816\n","테스트 정확도: 0.882\n"]}],"source":["vocab_size = 20000\n","sequence_length = 600\n","embed_dim = 256\n","num_heads = 2\n","dense_dim = 32\n","\n","inputs = keras.Input(shape=(None,), dtype=\"int64\")\n","x = PositionalEmbedding(sequence_length, vocab_size, embed_dim)(inputs)\n","x = TransformerEncoder(embed_dim, dense_dim, num_heads)(x)\n","x = layers.GlobalMaxPooling1D()(x)\n","x = layers.Dropout(0.5)(x)\n","outputs = layers.Dense(1, activation=\"sigmoid\")(x)\n","model = keras.Model(inputs, outputs)\n","model.compile(optimizer=\"rmsprop\",\n","              loss=\"binary_crossentropy\",\n","              metrics=[\"accuracy\"])\n","model.summary()\n","\n","callbacks = [\n","    keras.callbacks.ModelCheckpoint(\"full_transformer_encoder.keras\",\n","                                    save_best_only=True)\n","]\n","model.fit(int_train_ds, validation_data=int_val_ds, epochs=20, callbacks=callbacks)\n","model = keras.models.load_model(\n","    \"full_transformer_encoder.keras\",\n","    custom_objects={\"TransformerEncoder\": TransformerEncoder,\n","                    \"PositionalEmbedding\": PositionalEmbedding})\n","print(f\"테스트 정확도: {model.evaluate(int_test_ds)[1]:.3f}\")"]},{"cell_type":"markdown","metadata":{"id":"T17kNue242iy"},"source":["### BoW 모델 대신 언제 시퀀스 모델을 사용하나요?"]}],"metadata":{"accelerator":"GPU","colab":{"provenance":[{"file_id":"https://github.com/rickiepark/deep-learning-with-python-2nd/blob/main/chapter11_part03_transformer.ipynb","timestamp":1670845239560}],"machine_shape":"hm"},"kernelspec":{"display_name":"default:Python","language":"python","name":"conda-env-default-py"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.10"},"gpuClass":"standard"},"nbformat":4,"nbformat_minor":0}